# ü§ñ Primeros Pasos con IA Generativa

Este proyecto contiene experimentos iniciales con modelos de lenguaje de gran escala (LLMs) y tecnolog√≠as de inteligencia artificial generativa. Se enfoca en el aprendizaje pr√°ctico de conceptos fundamentales como tokenizaci√≥n, inferencia local de modelos y chat casual con diferentes arquitecturas de IA.

## üéØ Objetivo

Explorar los fundamentos de la inteligencia artificial generativa mediante:

- **Comprensi√≥n de tokenizaci√≥n**: C√≥mo los modelos procesan texto
- **Inferencia local de modelos**: Ejecuci√≥n de LLMs sin APIs externas
- **Comparaci√≥n de arquitecturas**: Evaluaci√≥n de diferentes familias de modelos
- **Optimizaci√≥n de recursos**: T√©cnicas de cuantizaci√≥n y gesti√≥n de memoria

## üõ†Ô∏è Tecnolog√≠as Utilizadas

- **Python 3.13+**
- **Transformers (Hugging Face)** - Para carga y ejecuci√≥n de modelos
- **PyTorch** - Framework de deep learning
- **BitsAndBytesConfig** - Para cuantizaci√≥n de modelos
- **Tiktoken** - Tokenizador de OpenAI
- **Google Colab** - Entorno de desarrollo (opcional)

## üìÅ Estructura del Proyecto

```text
02-genai-first-steps/
‚îú‚îÄ‚îÄ .python-version               # Versi√≥n de Python requerida
‚îú‚îÄ‚îÄ pyproject.toml               # Configuraci√≥n del proyecto
‚îú‚îÄ‚îÄ README.md                    # Este archivo
‚îú‚îÄ‚îÄ chatcasual_localllm.ipynb    # Chat con modelos locales
‚îî‚îÄ‚îÄ Tokenizers.ipynb             # Exploraci√≥n de tokenizaci√≥n
```

## üöÄ Experimentos Realizados

### 1. Chat Casual con LLMs Locales (chatcasual_localllm.ipynb)

Este notebook implementa un sistema de chat utilizando modelos de lenguaje ejecutados localmente en Google Colab.

#### Modelos Explorados

- **Microsoft Phi-3-mini-128k-instruct**: Modelo compacto y eficiente
- **Qwen2.5-7B-Instruct-1M**: Modelo de 7B par√°metros con contexto extendido
- **Falcon3-7B-Base**: Arquitectura Falcon para generaci√≥n de texto

#### Caracter√≠sticas T√©cnicas

- **Cuantizaci√≥n inteligente**: Uso de BitsAndBytesConfig para optimizar memoria
- **Streaming de respuestas**: Generaci√≥n de texto en tiempo real
- **Gesti√≥n de memoria GPU**: Configuraci√≥n autom√°tica seg√∫n recursos disponibles
- **Templates de conversaci√≥n**: Formatos espec√≠ficos para cada modelo

#### Configuraciones de Cuantizaci√≥n

```python
# Cuantizaci√≥n 4-bit para optimizar memoria
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4"
)
```

### 2. Exploraci√≥n de Tokenizaci√≥n (Tokenizers.ipynb)

An√°lisis profundo del proceso de tokenizaci√≥n y su impacto en el rendimiento de los modelos.

#### Conceptos Explorados

- **Tipos de tokenizaci√≥n**: Byte-pair encoding (BPE), WordPiece, SentencePiece
- **Vocabularios de modelos**: Comparaci√≥n entre diferentes arquitecturas
- **Longitud de contexto**: L√≠mites y optimizaciones
- **Encoding/Decoding**: Conversi√≥n bidireccional texto-tokens

#### Experimentos Pr√°cticos

- **An√°lisis comparativo**: Tokenizaci√≥n del mismo texto con diferentes modelos
- **C√°lculo de tokens**: Estimaci√≥n de costos y l√≠mites de contexto
- **Visualizaci√≥n**: Representaci√≥n gr√°fica del proceso de tokenizaci√≥n
- **Optimizaci√≥n**: T√©cnicas para reducir el n√∫mero de tokens

## ‚öôÔ∏è Configuraci√≥n

### 1. Requisitos del Sistema

```bash
# Memoria RAM recomendada: 16GB+
# GPU: CUDA compatible (opcional pero recomendado)
# Espacio en disco: 20GB+ para modelos
```

### 2. Instalaci√≥n de Dependencias

```bash
# Instalaci√≥n autom√°tica en notebooks
!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate

# O usando el entorno local
pip install transformers torch bitsandbytes accelerate
```

### 3. Configuraci√≥n de Hugging Face

```python
# Autenticaci√≥n (requerida para algunos modelos)
from huggingface_hub import login
login(token="tu_huggingface_token")
```

## üìä Experimentos y Resultados

### Comparaci√≥n de Modelos

| Modelo     | Par√°metros | Memoria (4-bit) | Velocidad | Calidad   |
| ---------- | ---------- | --------------- | --------- | --------- |
| Phi-3-mini | 3.8B       | ~2.5GB          | R√°pido    | Buena     |
| Qwen2.5    | 7B         | ~4.5GB          | Medio     | Excelente |
| Falcon3    | 7B         | ~4.5GB          | Medio     | Muy buena |

### An√°lisis de Tokenizaci√≥n

- **Eficiencia por idioma**: Comparaci√≥n espa√±ol vs ingl√©s
- **Patrones de token**: Identificaci√≥n de elementos comunes
- **Optimizaci√≥n de prompts**: T√©cnicas para reducir tokens
- **L√≠mites de contexto**: Gesti√≥n de conversaciones largas

## üéì Aprendizajes Clave

### Conceptos Fundamentales

1. **Tokenizaci√≥n es crucial**: Afecta directamente costos y rendimiento
2. **Cuantizaci√≥n permite democratizaci√≥n**: Modelos grandes en hardware limitado
3. **Cada modelo tiene personalidad**: Diferentes estilos y capacidades
4. **La optimizaci√≥n es esencial**: Balance entre calidad y recursos

### T√©cnicas Aprendidas

- **Gesti√≥n de memoria GPU**: Configuraci√≥n eficiente para diferentes tama√±os de modelo
- **Prompt engineering b√°sico**: Construcci√≥n de mensajes efectivos
- **Evaluaci√≥n cualitativa**: M√©todos para comparar salidas de modelos
- **Debugging de modelos**: Identificaci√≥n y resoluci√≥n de problemas comunes

## üîç Casos de Uso Pr√°cticos

1. **Asistente personal local**: Chat privado sin dependencias externas
2. **An√°lisis de texto**: Comprensi√≥n de estructura y contenido
3. **Prototipado r√°pido**: Experimentaci√≥n con diferentes modelos
4. **Educaci√≥n**: Comprensi√≥n pr√°ctica de conceptos de IA
5. **Investigaci√≥n**: Base para proyectos m√°s avanzados

## üöÄ Pr√≥ximos Pasos

### Mejoras Sugeridas

- **Fine-tuning**: Adaptaci√≥n de modelos a casos espec√≠ficos
- **Benchmarking autom√°tico**: Evaluaci√≥n sistem√°tica de rendimiento
- **Interface de usuario**: Desarrollo de chat web interactivo
- **Optimizaci√≥n avanzada**: T√©cnicas de compresi√≥n y aceleraci√≥n

### Experimentos Futuros

- **Modelos multimodales**: Integraci√≥n de texto e im√°genes
- **Agents**: Sistemas de IA con capacidad de acci√≥n
- **RAG avanzado**: Integraci√≥n con bases de conocimiento
- **Deploy en producci√≥n**: Sistemas escalables y robustos

## üìö Recursos Adicionales

### Documentaci√≥n

- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [BitsAndBytes Guide](https://github.com/TimDettmers/bitsandbytes)

### Modelos Recomendados

- **Para empezar**: Microsoft Phi-3-mini
- **Calidad/Rendimiento**: Qwen2.5-7B
- **Experimentaci√≥n**: Falcon3-7B
- **Producci√≥n**: Llama 2/3 (seg√∫n licencia)

## ‚ö†Ô∏è Consideraciones Importantes

### Recursos Computacionales

- **GPU recomendada**: Para modelos de 7B+ par√°metros
- **Tiempo de carga**: Los modelos pueden tardar varios minutos en cargar
- **Espacio de almacenamiento**: Los modelos ocupan varios GB

### Licencias y Uso

- **Revisar licencias**: Cada modelo tiene t√©rminos espec√≠ficos
- **Uso responsable**: Evitar contenido inapropiado o da√±ino
- **Privacidad**: Los modelos locales protegen datos sensibles
- **Costos**: Considerar uso de GPU en plataformas cloud

## üìù Notas del Curso

Este proyecto forma parte del m√≥dulo introductorio donde se cubren:

- **Fundamentos te√≥ricos**: Arquitecturas transformer y attention
- **Implementaci√≥n pr√°ctica**: Uso de librer√≠as y frameworks
- **Optimizaci√≥n**: T√©cnicas para recursos limitados
- **Evaluaci√≥n**: M√©todos para medir calidad y rendimiento
- **√âtica**: Consideraciones sobre sesgos y uso responsable
