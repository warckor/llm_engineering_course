{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da45bdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "490c00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_document = \"./docs/Resumen IC2.pdf\"\n",
    "persist_dir = \"./chroma_db\"\n",
    "\n",
    "GITHUB_MODEL = \"gpt-4o-mini\"\n",
    "GITHUB_EMBEDDINGS = \"text-embedding-3-small\"\n",
    "GITHUB_API_KEY = os.getenv(\"GITHUB_TOKEN\")\n",
    "GITHUB_MODEL_URL = os.getenv(\"GITHUB_MODEL_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530e65ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargadas 108 páginas del documento PDF.\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(pdf_document)\n",
    "pages = loader.load()\n",
    "\n",
    "# async for page in loader.alazy_load():\n",
    "#     pages.append(page)\n",
    "    \n",
    "print(f\"Cargadas {len(pages)} páginas del documento PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf03115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividido en 143 fragmentos de texto.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(pages)\n",
    "print(f\"Dividido en {len(chunks)} fragmentos de texto.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c99883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=GITHUB_MODEL, api_key=GITHUB_API_KEY, temperature=0, base_url=GITHUB_MODEL_URL)\n",
    "embeddings = OpenAIEmbeddings(model=GITHUB_EMBEDDINGS, api_key=GITHUB_API_KEY, base_url=GITHUB_MODEL_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69b698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "    Chroma(persist_directory=persist_dir, embedding_function=embeddings).delete_collection()\n",
    "    \n",
    "    print(f\"Se eliminó la colección existente en {persist_dir}\")\n",
    "\n",
    "try:\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=chunks,  # Ahora usamos todos los chunks\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_dir,\n",
    "        collection_name=\"openai_collection\"\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0310513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo: Para simplificar las operaciones que se re alizan en la etapa de extracción y en la decodificación, el \n",
      "PowerPC970 cuenta con una etapa de predecodificación situada delate de la I -caché y del buffer de prefetch. \n",
      "Esta etapa se ocupa de añadir 5 bits a cada instrucción, que se utilizan posteriormente en la etapa de Fetch para \n",
      "determinar el tipo de instrucciones de salto y proceder o no a su especulación. \n",
      " \n",
      " \n",
      " \n",
      "2.4.2 Traducción de instrucciones \n",
      " \n",
      "Es el proceso por el que una instrucción RISC o CISC se convierte en un grupo equivalente de instrucciones \n",
      "básicas RISC. Estas operaciones se conocen como: \n",
      " \n",
      "- Microoperaciones (micro-ops) en terminología Intel. \n",
      "- Operaciones internas (IOPs - Internal Operations) en terminología PowerPC \n",
      "- ROPs (RIPS operations) en terminología AMD. \n",
      " \n",
      "Esta técnica se utiliza para redu cir la complejidad de determinadas instrucciones. La idea que subyace es \n",
      "simplificar el repertorio original de instrucciones para que su procesamiento hardware se pueda realizar \n",
      "directamente en el procesador. \n",
      " \n",
      "Ejemplo: La arquitectura CISC que emplea la traducción de instrucciones es la Intel Core Microarchitecture. \n",
      "La etapa de decodificación consta de 4 decodificadores (uno generalizado y lento y tres restringidos pero \n",
      "rápidos) y un generador de microcódigo que permiten decodificar 4 instrucciones de long itud variable en \n",
      "paralelo. \n",
      " \n",
      "El primero de los 4 decodificadores, D0, puede manipular cualquier instrucción. Por su parte, los 3 restantes \n",
      "están limitados a instrucciones sencillas que generan una única micro -op. La secuencia de micro-ops obtenida \n",
      "se envía en grupos ordenados de 7 a un buffer de micro-ops desde donde pasan en grupos de 4 al RAT (Register \n",
      "Alias Table) para el renombramiento de registros y posteriormente al buffer de reordenamiento (ROB, ReOrder \n",
      "Buffer).\n"
     ]
    }
   ],
   "source": [
    "print(retriever.invoke(\"¿Qué es RISC?\")[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab308c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aware_retriever_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Genera una consulta de búsqueda para recuperar información relevante basada en la conversación.\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"\"\"\n",
    "            Responde brevemente a la pregunta del usuario basándote en el contexto proporcionado. \n",
    "            Si no sabes la respuesta, di 'No lo sé' o 'No tengo información al respecto.'\n",
    "            No inventes respuestas.\n",
    "            Contexto: {context}\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bcce7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Template del prompt del retriever:\n",
      " Genera una consulta de búsqueda para recuperar información relevante basada en la conversación. \n",
      "\n",
      "- Variables del prompt del retriever:\n",
      " ['chat_history', 'input'] \n",
      "\n",
      "-------------------------------------------------- \n",
      "\n",
      "- Template del prompt de QA:\n",
      " \n",
      "                Responde brevemente a la pregunta del usuario basándote en el contexto proporcionado. \n",
      "                Si no sabes la respuesta, di 'No lo sé' o 'No tengo información al respecto.'\n",
      "                No inventes respuestas.\n",
      "                Contexto: {context}\n",
      "             \n",
      "\n",
      "- Variables del prompt de QA:\n",
      " ['chat_history', 'context', 'input'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"- Template del prompt del retriever:\\n\", aware_retriever_prompt.messages[0].prompt.template, \"\\n\")\n",
    "print(\"- Variables del prompt del retriever:\\n\", aware_retriever_prompt.input_variables, \"\\n\")\n",
    "print(\"-\" * 50, \"\\n\")\n",
    "print(\"- Template del prompt de QA:\\n\", qa_prompt.messages[0].prompt.template, \"\\n\")\n",
    "print(\"- Variables del prompt de QA:\\n\", qa_prompt.input_variables, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277b744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables del aware_retriever_prompt: ['chat_history', 'input']\n",
      "Variables del prompt corregido: ['chat_history', 'input']\n",
      "✅ History-aware retriever creado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# Ahora crear el history aware retriever\n",
    "try:\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        retriever=retriever,\n",
    "        llm=llm,\n",
    "        prompt=aware_retriever_prompt\n",
    "    )\n",
    "    print(\"✅ History-aware retriever creado exitosamente!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e063f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "| ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7485636567a0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"\\n                Responde brevemente a la pregunta del usuario basándote en el contexto proporcionado. \\n                Si no sabes la respuesta, di 'No lo sé' o 'No tengo información al respecto.'\\n                No inventes respuestas.\\n                Contexto: {context}\\n            \"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x74853be70d70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x74853be717f0>, root_client=<openai.OpenAI object at 0x74853c57e3c0>, root_async_client=<openai.AsyncOpenAI object at 0x74853be71550>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://models.github.ai/inference')\n",
      "| StrOutputParser() kwargs={} config={'run_name': 'stuff_documents_chain'} config_factories=[]\n"
     ]
    }
   ],
   "source": [
    "documents_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)\n",
    "print(documents_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "898361db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableAssign(mapper={\n",
      "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
      "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x74853be71e80>, search_kwargs={'k': 3}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7485636567a0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Genera una consulta de búsqueda para recuperar información relevante basada en la conversación.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
      "           | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x74853be70d70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x74853be717f0>, root_client=<openai.OpenAI object at 0x74853c57e3c0>, root_async_client=<openai.AsyncOpenAI object at 0x74853be71550>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://models.github.ai/inference')\n",
      "           | StrOutputParser()\n",
      "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x74853be71e80>, search_kwargs={'k': 3})), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
      "})\n",
      "| RunnableAssign(mapper={\n",
      "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "              context: RunnableLambda(format_docs)\n",
      "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "            | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7485636567a0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"\\n                Responde brevemente a la pregunta del usuario basándote en el contexto proporcionado. \\n                Si no sabes la respuesta, di 'No lo sé' o 'No tengo información al respecto.'\\n                No inventes respuestas.\\n                Contexto: {context}\\n            \"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
      "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x74853be70d70>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x74853be717f0>, root_client=<openai.OpenAI object at 0x74853c57e3c0>, root_async_client=<openai.AsyncOpenAI object at 0x74853be71550>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://models.github.ai/inference')\n",
      "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
      "  }) kwargs={} config={'run_name': 'retrieval_chain'} config_factories=[]\n"
     ]
    }
   ],
   "source": [
    "retrieval_chain = create_retrieval_chain(\n",
    "    retriever=history_aware_retriever,\n",
    "    combine_docs_chain=documents_chain\n",
    ")\n",
    "print(retrieval_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32461afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "def ask_to_chatbot(question: str):\n",
    "    inputs = {\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    }\n",
    "    result = retrieval_chain.invoke(inputs)\n",
    "    answer = result[\"answer\"]\n",
    "    chat_history.append(HumanMessage(content=question))\n",
    "    chat_history.append(AIMessage(content=answer))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d1c00bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RISC (Reduced Instruction Set Computer) es una arquitectura de procesador que se caracteriza por un conjunto de instrucciones sencillas y de formato fijo, lo que permite una ejecución más rápida y eficiente. Su diseño busca reducir la cantidad de accesos a memoria mediante un aumento en la cantidad de registros internos en el procesador y facilita la implementación de la ejecución segmentada de instrucciones (pipeline). Esto resulta en un hardware más simple y un mayor rendimiento en comparación con arquitecturas más complejas como CISC (Complex Instruction Set Computer).\n",
      "CISC (Complex Instruction Set Computer) es una arquitectura de procesador que se caracteriza por tener un conjunto de instrucciones más complejo y potente, capaz de realizar múltiples operaciones en una sola instrucción. Esta arquitectura fue diseñada para reducir el tamaño de los programas y la cantidad de instrucciones que debían almacenarse, lo que era importante en los primeros computadores con memoria limitada. Sin embargo, las instrucciones en CISC requieren varios ciclos de reloj para ser ejecutadas, lo que puede dificultar el paralelismo y aumentar la complejidad del hardware.\n",
      "Me preguntaste qué es CISC.\n",
      "Después de preguntarme por RISC, me preguntaste qué es CISC.\n"
     ]
    }
   ],
   "source": [
    "print(ask_to_chatbot(\"¿Qué es RISC?\"))\n",
    "print(ask_to_chatbot(\"¿Y CISC?\"))\n",
    "print(ask_to_chatbot(\"¿Qué te pregunté antes?\"))\n",
    "print(ask_to_chatbot(\"¿Y después de preguntarte por RISC, por qué te pregunté?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
