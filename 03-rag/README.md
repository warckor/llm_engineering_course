# üìö RAG - Retrieval Augmented Generation

Este proyecto implementa un sistema de **Retrieval Augmented Generation (RAG)** que permite consultar documentos PDF utilizando t√©cnicas de procesamiento de lenguaje natural y embeddings. El sistema divide documentos largos en fragmentos manejables, los convierte en representaciones vectoriales y permite realizar consultas inteligentes sobre el contenido.

## üéØ Objetivo

Desarrollar un sistema RAG completo que permita:

- **Procesamiento de documentos PDF**: Extracci√≥n y an√°lisis de texto
- **Segmentaci√≥n inteligente**: Divisi√≥n de contenido en chunks sem√°nticamente coherentes
- **Generaci√≥n de embeddings**: Representaci√≥n vectorial del contenido
- **Consultas contextuales**: Respuestas basadas en contenido espec√≠fico del documento
- **Integraci√≥n con LLMs**: Uso de modelos de lenguaje para respuestas naturales

## üõ†Ô∏è Tecnolog√≠as Utilizadas

- **Python 3.13+**
- **LangChain Community** - Para carga y procesamiento de documentos
- **PyPDF** - Extracci√≥n de texto de archivos PDF
- **OpenAI API** - Modelos de lenguaje y embeddings
- **Tiktoken** - Tokenizaci√≥n y conteo de tokens
- **RecursiveCharacterTextSplitter** - Segmentaci√≥n inteligente de texto
- **python-dotenv** - Gesti√≥n de variables de entorno

## üìÅ Estructura del Proyecto

```text
03-rag/
‚îú‚îÄ‚îÄ .env                        # Variables de entorno (API keys)
‚îú‚îÄ‚îÄ .python-version            # Versi√≥n de Python requerida
‚îú‚îÄ‚îÄ pyproject.toml             # Configuraci√≥n del proyecto y dependencias
‚îú‚îÄ‚îÄ uv.lock                    # Lock file para reproducibilidad
‚îú‚îÄ‚îÄ README.md                  # Este archivo
‚îú‚îÄ‚îÄ split-text.ipynb           # Implementaci√≥n del sistema RAG
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ Resoluci√≥n ex√°menes TLP.pdf  # Documento de prueba
```

## üöÄ Funcionalidades Principales

### 1. Carga de Documentos PDF

Utiliza **PyPDFLoader** de LangChain para:

- **Extracci√≥n autom√°tica**: Texto de todas las p√°ginas del PDF
- **Preservaci√≥n de metadatos**: Informaci√≥n de p√°ginas y estructura
- **Carga as√≠ncrona**: Procesamiento eficiente de documentos grandes
- **Gesti√≥n de errores**: Manejo robusto de archivos corruptos o protegidos

```python
# Ejemplo de carga de documento
loader = PyPDFLoader("./docs/documento.pdf")
pages = []
async for page in loader.alazy_load():
    pages.append(page)
```

### 2. Segmentaci√≥n Inteligente de Texto

Implementa **RecursiveCharacterTextSplitter** para:

- **Divisi√≥n contextual**: Mantiene coherencia sem√°ntica en los fragmentos
- **Tama√±o optimizado**: Chunks de 1000 caracteres con overlap de 200
- **Preservaci√≥n de estructura**: Respeta p√°rrafos y secciones
- **Flexibilidad configurable**: Par√°metros ajustables seg√∫n el tipo de documento

```python
# Configuraci√≥n del splitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, 
    chunk_overlap=200
)
chunks = splitter.split_documents(pages)
```

### 3. Generaci√≥n de Embeddings

Integraci√≥n con modelos de embedding para:

- **Representaci√≥n vectorial**: Conversi√≥n de texto a vectores num√©ricos
- **Similitud sem√°ntica**: B√∫squeda por significado, no solo palabras clave
- **Modelos variados**: Soporte para diferentes proveedores (OpenAI, Cohere)
- **Optimizaci√≥n de costos**: Procesamiento por lotes eficiente

### 4. Sistema de Consultas Contextuales

Implementa un pipeline completo de RAG:

- **B√∫squeda sem√°ntica**: Encuentra fragmentos relevantes autom√°ticamente
- **Contexto enriquecido**: Combina m√∫ltiples chunks para respuestas completas
- **Respuestas naturales**: Genera texto fluido basado en el contenido encontrado
- **Trazabilidad**: Identifica las fuentes utilizadas en cada respuesta

## ‚öôÔ∏è Configuraci√≥n

### 1. Variables de Entorno

Crear un archivo `.env` en la ra√≠z del proyecto:

```env
# Para GitHub Models (alternativa gratuita)
GITHUB_TOKEN=tu_token_de_github
GITHUB_MODEL_URL=https://models.inference.ai.azure.com

# O para OpenAI oficial
OPENAI_API_KEY=tu_api_key_de_openai
OPENAI_BASE_URL=https://api.openai.com/v1

# Configuraci√≥n de modelos
MODEL=cohere/cohere-command-a
EMBEDDING_MODEL=cohere/Cohere-embed-v3-multilingual
```

### 2. Instalaci√≥n de Dependencias

```bash
# Usando uv (recomendado)
uv sync

# O usando pip tradicional
pip install langchain langchain-community pypdf openai tiktoken python-dotenv
```

### 3. Preparaci√≥n de Documentos

```bash
# Crear directorio de documentos
mkdir -p docs/

# Copiar archivos PDF al directorio
cp tu_documento.pdf docs/
```

## üìä Flujo de Trabajo RAG

### Fase 1: Procesamiento del Documento

1. **Carga**: Extracci√≥n de texto del PDF p√°gina por p√°gina
2. **An√°lisis**: Identificaci√≥n de estructura y metadatos
3. **Segmentaci√≥n**: Divisi√≥n en chunks sem√°nticamente coherentes
4. **Validaci√≥n**: Verificaci√≥n de integridad y calidad de los fragmentos

### Fase 2: Generaci√≥n de Embeddings

1. **Preparaci√≥n**: Limpieza y normalizaci√≥n de texto
2. **Embedding**: Conversi√≥n a representaciones vectoriales
3. **Almacenamiento**: Persistencia de vectores y metadatos
4. **Indexaci√≥n**: Preparaci√≥n para b√∫squedas eficientes

### Fase 3: Consulta y Respuesta

1. **Query processing**: An√°lisis de la consulta del usuario
2. **Retrieval**: B√∫squeda de fragmentos m√°s relevantes
3. **Context building**: Construcci√≥n del contexto para el LLM
4. **Generation**: Generaci√≥n de respuesta natural y coherente

## üîç An√°lisis de Rendimiento

### M√©tricas de Tokenizaci√≥n

El sistema incluye an√°lisis detallado de tokens para:

- **Optimizaci√≥n de costos**: C√°lculo preciso de tokens consumidos
- **Gesti√≥n de contexto**: Verificaci√≥n de l√≠mites de modelos
- **Eficiencia de prompts**: Optimizaci√≥n de system prompts
- **Monitoreo de uso**: Seguimiento de consumo de API

```python
# Ejemplo de an√°lisis de tokens
encoding = tiktoken.get_encoding("cl100k_base")
system_prompt_tokens = len(encoding.encode(system_prompt))
print(f"Tokens en system prompt: {system_prompt_tokens}")
```

### Configuraci√≥n de Modelos

#### Modelos de Lenguaje Probados

- **Cohere Command-A**: Excelente para tareas de RAG en espa√±ol
- **OpenAI GPT-4.1-mini**: Modelo compacto y eficiente
- **Alternativas locales**: Compatible con LMStudio y Ollama

#### Modelos de Embedding

- **Cohere Embed v3 Multilingual**: Optimizado para idiomas m√∫ltiples
- **OpenAI text-embedding-3-small**: Eficiente y econ√≥mico
- **Alternativas locales**: Sentence Transformers

## üéì Casos de Uso Implementados

### 1. An√°lisis de Ex√°menes Acad√©micos

El proyecto incluye un ejemplo pr√°ctico con:

- **Documento**: Resoluci√≥n de ex√°menes TLP (Teor√≠a de Lenguajes de Programaci√≥n)
- **Consultas t√≠picas**: B√∫squeda de soluciones espec√≠ficas
- **Respuestas contextuales**: Explicaciones detalladas basadas en el contenido
- **Trazabilidad**: Referencias exactas a p√°ginas y secciones

### 2. Consultor√≠a Automatizada

Potencial para:

- **Base de conocimiento**: Consultas sobre documentaci√≥n t√©cnica
- **Soporte al cliente**: Respuestas autom√°ticas basadas en manuales
- **Investigaci√≥n acad√©mica**: An√°lisis de papers y publicaciones
- **An√°lisis legal**: Consultas sobre documentos normativos

## üöÄ Caracter√≠sticas Avanzadas

### Gesti√≥n Inteligente de Contexto

- **L√≠mites din√°micos**: Adaptaci√≥n autom√°tica a l√≠mites de modelo
- **Priorizaci√≥n**: Selecci√≥n de chunks m√°s relevantes cuando hay l√≠mites
- **Overlap estrat√©gico**: Preservaci√≥n de coherencia entre fragmentos
- **Metadata enrichment**: Informaci√≥n adicional sobre ubicaci√≥n y relevancia

### Optimizaci√≥n de Rendimiento

- **Procesamiento por lotes**: Embedding m√∫ltiples chunks simult√°neamente
- **Cach√© inteligente**: Reutilizaci√≥n de embeddings previamente calculados
- **Lazy loading**: Carga de documentos bajo demanda
- **Memory management**: Gesti√≥n eficiente de memoria para documentos grandes

## üìà M√©tricas y Evaluaci√≥n

### Calidad de Respuestas

- **Relevancia**: ¬øLa respuesta aborda la pregunta espec√≠fica?
- **Completitud**: ¬øSe incluye informaci√≥n suficiente?
- **Precisi√≥n**: ¬øLos datos citados son correctos?
- **Coherencia**: ¬øLa respuesta es l√≥gica y bien estructurada?

### Eficiencia del Sistema

- **Tiempo de respuesta**: Latencia desde consulta hasta respuesta
- **Uso de tokens**: Optimizaci√≥n de costos de API
- **Calidad de retrieval**: Precisi√≥n en la selecci√≥n de chunks
- **Escalabilidad**: Rendimiento con documentos de diferentes tama√±os

## üîÑ Integraci√≥n y Extensibilidad

### APIs y Servicios

- **GitHub Models**: Acceso gratuito a modelos de IA para desarrollo
- **OpenAI API**: Modelos premium para producci√≥n
- **Cohere**: Especializaci√≥n en modelos multiling√ºes
- **Servicios locales**: LMStudio, Ollama para despliegues privados

### Extensiones Futuras

- **Vector databases**: Chroma, Pinecone, Weaviate para escalabilidad
- **M√∫ltiples formatos**: Word, Excel, HTML, etc.
- **Interfaces de usuario**: Streamlit, FastAPI para demos
- **Evaluaci√≥n autom√°tica**: M√©tricas de calidad automatizadas

## ü§ù Aprendizajes del Curso

### Conceptos Fundamentales

1. **RAG vs Fine-tuning**: Cu√°ndo usar cada aproximaci√≥n
2. **Chunking strategies**: Impacto en la calidad de respuestas
3. **Embedding selection**: Criterios para elegir modelos de embedding
4. **Prompt engineering**: Optimizaci√≥n de system prompts para RAG
5. **Cost management**: Estrategias para controlar gastos de API

### T√©cnicas Avanzadas

- **Hybrid search**: Combinaci√≥n de b√∫squeda sem√°ntica y tradicional
- **Re-ranking**: Mejora de resultados con modelos especializados
- **Query expansion**: Enriquecimiento de consultas para mejor retrieval
- **Context compression**: Optimizaci√≥n de contexto para l√≠mites de token

## ‚ö†Ô∏è Consideraciones Importantes

### Privacidad y Seguridad

- **Datos sensibles**: Cuidado con informaci√≥n confidencial en APIs externas
- **Compliance**: Consideraciones GDPR y regulaciones locales
- **Modelos locales**: Alternativas para m√°xima privacidad
- **Audit trails**: Registro de consultas y respuestas

### Limitaciones Actuales

- **Tama√±o de documentos**: Documentos muy grandes pueden requerir optimizaci√≥n
- **Formato de archivos**: Limitado a PDF (expandible a otros formatos)
- **Idioma**: Optimizado para espa√±ol, pero funciona con otros idiomas
- **Actualizaci√≥n**: Los documentos requieren reprocesamiento tras cambios

## üìö Recursos Adicionales

### Documentaci√≥n T√©cnica

- [LangChain Documentation](https://python.langchain.com/docs/)
- [OpenAI API Reference](https://platform.openai.com/docs/)
- [Cohere Documentation](https://docs.cohere.com/)

### Papers y Referencias

- "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
- "Dense Passage Retrieval for Open-Domain Question Answering"
- "In-Context Retrieval-Augmented Language Models"

### Herramientas Recomendadas

- **Vector databases**: Chroma, Pinecone, Weaviate
- **Evaluation**: RAGAS, TruLens
- **Monitoring**: LangSmith, Weights & Biases
- **UI/UX**: Streamlit, Gradio, Chainlit

Este proyecto representa una implementaci√≥n completa y educativa de un sistema RAG, proporcionando una base s√≥lida para aplicaciones m√°s avanzadas de inteligencia artificial aplicada al procesamiento de documentos.
